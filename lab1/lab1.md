Теория
Naive Bayes Classifier
Байесовский классификатор — широкий класс алгоритмов классификации, основанный на принципе максимума апостериорной вероятности. Для классифицируемого объекта вычисляются функции правдоподобия каждого из классов, по ним вычисляются апостериорные вероятности классов. Объект относится к тому классу, для которого апостериорная вероятность максимальна.
Наивный байесовский классификатор - частный случай байесовского классификатора, основывающийся на том, что все признаки, описывающие объект, независимы друг от друга.
Пусть   — множество описаний объектов,   — множество номеров (или наименований) классов. На множестве пар «объект, класс»  определена вероятностная мера  . Имеется конечная обучающая выборка независимых наблюдений  , полученных согласно вероятностной мере  .
Задача классификации заключается в том, чтобы построить алгоритм  , способный классифицировать произвольный объект  .
В байесовской теории классификации эта задача разделяется на две.
■	Построение оптимального классификатора при известных плотностях классов. Эта подзадача имеет простое и окончательное решение.
■	Восстановление плотностей классов по обучающей выборке. В этой подзадаче сосредоточена основная сложность байесовского подхода к классификации.
Построение классификатора при известных плотностях классов
Пусть для каждого класса   известна априорная вероятность   того, что появится объект класса  , и плотности распределения  каждого из классов, называемые также функциями правдоподобия классов. Требуется построить алгоритм классификации  , доставляющий минимальное значение функционалу среднего риска.
Средний риск опредеяется как математическое ожидание ошибки:
 
где   — цена ошибки или штраф за отнесение объекта класса   к какому-либо другому классу.
Теорема. Решением этой задачи является алгоритм
 
Значение   интерпретируется как апостериорная вероятность того, что объект   принадлежит классу  .
Если классы равнозначимы,  , то объект   просто относится к классу с наибольшим значением плотности распределения в точке  .
Восстановление плотностей классов по обучающей выборке
По заданной подвыборке объектов класса   построить эмпирические оценки априорных вероятностей   и функций правдоподобия  .
В качестве оценки априорных вероятностей берут, как правило, долю объектов данного класса в обучающей выборке.
Восстановление плотностей (функций правдоподобия каждого из классов) является наиболее трудной задачей. Наиболее распространены три подхода: параметрический, непараметрический и разделение смеси вероятностных распределений. Третий подход занимает промежуточное положение между первыми двумя, и в определённом смысле является наиболее общим.
■	Параметрическое восстановление плотности при дополнительном предположении, что плотности нормальные (гауссовские), приводит к нормальному дискриминантному анализу и линейному дискриминанту Фишера.
■	Непараметрическое восстановление плотности приводит, в частности, к методу парзеновского окна.
■	Разделение смеси распределений может быть сделано с помощью EM-алгоритма. Дополнительное предположение, что плотности компонент смеси являются радиальными функциями, приводит к методу радиальных базисных функций. Обычно в качестве компонент смеси берут, опять-таки, гауссовские плотности.
Таким образом, формула байесовского классификатора приводит к большому разнообразию байесовских алгоритмов, отличающихся только способом восстановления плотностей.
K Nearest Neighbor Classifier
Метод ближайших соседей — простейший метрический классификатор, основанный на оценивании сходства объектов. Классифицируемый объект относится к тому классу, которому принадлежат ближайшие к нему объекты обучающей выборки.
Метод ближайшего соседа является, пожалуй, самым простым алгоритмом классификации. Классифицируемый объект   относится к тому классу  , которому принадлежит ближайший объект обучающей выборки  .
Метод   ближайших соседей. Для повышения надёжности классификации объект относится к тому классу, которому принадлежит большинство из его соседей —   ближайших к нему объектов обучающей выборки  . В задачах с двумя классами число соседей берут нечётным, чтобы не возникало ситуаций неоднозначности, когда одинаковое число соседей принадлежат разным классам.
Пусть задана обучающая выборка пар «объект-ответ»  .
Пусть на множестве объектов задана функция расстояния  . Эта функция должна быть достаточно адекватной моделью сходства объектов. Чем больше значение этой функции, тем менее схожими являются два объекта  .
Для произвольного объекта   расположим объекты обучающей выборки   в порядке возрастания расстояний до  :
 
где через   обозначается тот объект обучающей выборки, который является  -м соседом объекта  . Аналогичное обозначение введём и для ответа на  -м соседе:  . Таким образом, произвольный объект   порождает свою перенумерацию выборки. В наиболее общем виде алгоритм ближайших соседей есть
 
где   — заданная весовая функция, которая оценивает степень важности  -го соседа для классификации объекта  . Естественно полагать, что эта функция неотрицательна и не возрастает по  .
По-разному задавая весовую функцию, можно получать различные варианты метода ближайших соседей.
■	  — простейший метод ближайшего соседа;
■	  — метод   ближайших соседей;
■	  — метод   экспоненциально взвешенных ближайших соседей, где предполагается  ;
■	  — метод парзеновского окна фиксированной ширины  ;
■	  — метод парзеновского окна переменной ширины;
■	  — метод потенциальных функций, в котором ширина окна   зависит не от классифицируемого объекта, а от обучающего объекта  .
Здесь   — заданная неотрицательная монотонно невозрастающая функция на  , ядро сглаживания.
Задание
1.	На языке Python программно реализовать два метрических алгоритма классификации: Naive Bayes и K Nearest Neighbours
2.	Сравнить работу реализованных алгоритмов с библиотечными из scikit-learn
3.	Для тренировки, теста и валидации использовать один из предложенных датасетов (либо найти самостоятельно и внести в таблицу)
4.	Сформировать краткий отчет (постановка задачи, реализация, эксперимент с данными, полученные характеристики, вывод
